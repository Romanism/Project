{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['CAPSULE_TEXT'])\n",
    "column = le.transform(train['CAPSULE_TEXT'])\n",
    "train['CAPSULE_TEXT'] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['GENRE_NAME'])\n",
    "column = le.transform(train['GENRE_NAME'])\n",
    "train['GENRE_NAME'] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['SEX_ID'])\n",
    "column = le.transform(train['SEX_ID'])\n",
    "train['SEX_ID'] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['large_area_name'])\n",
    "column = le.transform(train['large_area_name'])\n",
    "train['large_area_name'] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['ken_name'])\n",
    "column = le.transform(train['ken_name'])\n",
    "train['ken_name'] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['small_area_name'])\n",
    "column = le.transform(train['small_area_name'])\n",
    "train['small_area_name'] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF Factor</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.603343e+01</td>\n",
       "      <td>CAPSULE_TEXT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.991143e+01</td>\n",
       "      <td>GENRE_NAME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.920396e+00</td>\n",
       "      <td>SEX_ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.806329e+01</td>\n",
       "      <td>USABLE_DATE_MON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.347335e+01</td>\n",
       "      <td>USABLE_DATE_TUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.057954e+02</td>\n",
       "      <td>USABLE_DATE_WED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.367520e+02</td>\n",
       "      <td>USABLE_DATE_THU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.510812e+01</td>\n",
       "      <td>USABLE_DATE_FRI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.508113e+01</td>\n",
       "      <td>USABLE_DATE_SAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.329885e+01</td>\n",
       "      <td>USABLE_DATE_SUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.049509e+01</td>\n",
       "      <td>USABLE_DATE_HOLIDAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.152730e+01</td>\n",
       "      <td>USABLE_DATE_BEFORE_HOLIDAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.038989e+01</td>\n",
       "      <td>large_area_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.092425e+00</td>\n",
       "      <td>ken_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.944137e+00</td>\n",
       "      <td>small_area_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.959768e+09</td>\n",
       "      <td>DISPFROM_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.396254e+02</td>\n",
       "      <td>DISPFROM_month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.355234e+00</td>\n",
       "      <td>DISPFROM_weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.528136e+03</td>\n",
       "      <td>DISPFROM_hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.969898e+09</td>\n",
       "      <td>DISPEND_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.472590e+02</td>\n",
       "      <td>DISPEND_month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.766961e+00</td>\n",
       "      <td>DISPEND_weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.899069e+03</td>\n",
       "      <td>DISPEND_hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.473608e+07</td>\n",
       "      <td>REG_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8.855452e+00</td>\n",
       "      <td>REG_month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.263598e+00</td>\n",
       "      <td>REG_weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.536256e+00</td>\n",
       "      <td>REG_hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.303744e+01</td>\n",
       "      <td>PRICE_RATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.043242e+01</td>\n",
       "      <td>CATALOG_PRICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.111342e+01</td>\n",
       "      <td>DISCOUNT_PRICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.784566e+00</td>\n",
       "      <td>DISPPERIOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.268797e+01</td>\n",
       "      <td>VALIDPERIOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.981162e+01</td>\n",
       "      <td>AGE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      VIF Factor                    features\n",
       "0   7.603343e+01                CAPSULE_TEXT\n",
       "1   5.991143e+01                  GENRE_NAME\n",
       "2   1.920396e+00                      SEX_ID\n",
       "3   3.806329e+01             USABLE_DATE_MON\n",
       "4   5.347335e+01             USABLE_DATE_TUE\n",
       "5   1.057954e+02             USABLE_DATE_WED\n",
       "6   1.367520e+02             USABLE_DATE_THU\n",
       "7   4.510812e+01             USABLE_DATE_FRI\n",
       "8   2.508113e+01             USABLE_DATE_SAT\n",
       "9   2.329885e+01             USABLE_DATE_SUN\n",
       "10  3.049509e+01         USABLE_DATE_HOLIDAY\n",
       "11  3.152730e+01  USABLE_DATE_BEFORE_HOLIDAY\n",
       "12  1.038989e+01             large_area_name\n",
       "13  7.092425e+00                    ken_name\n",
       "14  3.944137e+00             small_area_name\n",
       "15  6.959768e+09               DISPFROM_year\n",
       "16  6.396254e+02              DISPFROM_month\n",
       "17  3.355234e+00            DISPFROM_weekday\n",
       "18  2.528136e+03               DISPFROM_hour\n",
       "19  6.969898e+09                DISPEND_year\n",
       "20  6.472590e+02               DISPEND_month\n",
       "21  3.766961e+00             DISPEND_weekday\n",
       "22  6.899069e+03                DISPEND_hour\n",
       "23  2.473608e+07                    REG_year\n",
       "24  8.855452e+00                   REG_month\n",
       "25  3.263598e+00                 REG_weekday\n",
       "26  6.536256e+00                    REG_hour\n",
       "27  4.303744e+01                  PRICE_RATE\n",
       "28  1.043242e+01               CATALOG_PRICE\n",
       "29  1.111342e+01              DISCOUNT_PRICE\n",
       "30  4.784566e+00                  DISPPERIOD\n",
       "31  1.268797e+01                 VALIDPERIOD\n",
       "32  1.981162e+01                         AGE"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "train_vif = train.drop(['USER_ID_hash', 'COUPON_ID_hash', 'PURCHASE_FLG'], axis = 1)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(train_vif.values, i) for i in range(train_vif.shape[1])]\n",
    "vif[\"features\"] = train_vif.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Select columns & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['USER_ID_hash', 'COUPON_ID_hash', 'CAPSULE_TEXT', 'GENRE_NAME',\n",
       "       'SEX_ID', 'USABLE_DATE_MON', 'USABLE_DATE_TUE', 'USABLE_DATE_WED',\n",
       "       'USABLE_DATE_THU', 'USABLE_DATE_FRI', 'USABLE_DATE_SAT',\n",
       "       'USABLE_DATE_SUN', 'USABLE_DATE_HOLIDAY', 'USABLE_DATE_BEFORE_HOLIDAY',\n",
       "       'large_area_name', 'ken_name', 'small_area_name', 'DISPFROM_year',\n",
       "       'DISPFROM_month', 'DISPFROM_weekday', 'DISPFROM_hour', 'DISPEND_year',\n",
       "       'DISPEND_month', 'DISPEND_weekday', 'DISPEND_hour', 'REG_year',\n",
       "       'REG_month', 'REG_weekday', 'REG_hour', 'PRICE_RATE', 'CATALOG_PRICE',\n",
       "       'DISCOUNT_PRICE', 'DISPPERIOD', 'VALIDPERIOD', 'AGE', 'PURCHASE_FLG'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['USER_ID_hash', 'COUPON_ID_hash', 'CAPSULE_TEXT', 'GENRE_NAME',\n",
       "       'SEX_ID', 'USABLE_DATE_MON', 'USABLE_DATE_TUE', 'USABLE_DATE_WED',\n",
       "       'USABLE_DATE_THU', 'USABLE_DATE_FRI', 'USABLE_DATE_SAT',\n",
       "       'USABLE_DATE_SUN', 'USABLE_DATE_HOLIDAY', 'USABLE_DATE_BEFORE_HOLIDAY',\n",
       "       'large_area_name', 'ken_name', 'small_area_name', 'DISPFROM_year',\n",
       "       'DISPFROM_month', 'DISPFROM_weekday', 'DISPFROM_hour', 'DISPEND_year',\n",
       "       'DISPEND_month', 'DISPEND_weekday', 'DISPEND_hour', 'REG_year',\n",
       "       'REG_month', 'REG_weekday', 'REG_hour', 'PRICE_RATE', 'CATALOG_PRICE',\n",
       "       'DISCOUNT_PRICE', 'DISPPERIOD', 'VALIDPERIOD', 'AGE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns\n",
    "category = ['CAPSULE_TEXT', 'GENRE_NAME', 'SEX_ID', 'USABLE_DATE_MON', 'USABLE_DATE_TUE',   \n",
    "            'USABLE_DATE_FRI', 'USABLE_DATE_SAT', 'USABLE_DATE_SUN', 'USABLE_DATE_HOLIDAY', 'USABLE_DATE_BEFORE_HOLIDAY',\n",
    "            'large_area_name', 'ken_name', 'small_area_name', 'DISPFROM_weekday', 'DISPEND_weekday', \n",
    "            'REG_month', 'REG_weekday', 'REG_hour']\n",
    "continuous = ['PRICE_RATE', 'CATALOG_PRICE', 'DISCOUNT_PRICE', 'DISPPERIOD', 'VALIDPERIOD', 'AGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make train/test data\n",
    "\n",
    "train_cols, test_cols = [], []\n",
    "\n",
    "# category\n",
    "for cat in category:\n",
    "    train_tok, test_tok = category_to_ohe(train[cat],test[cat])\n",
    "    train_cols.append(train_tok)\n",
    "    test_cols.append(test_tok)    \n",
    "\n",
    "# continuous\n",
    "for con in continuous:\n",
    "    train_cols.append(train[con].values.reshape(len(train),1))\n",
    "    test_cols.append(test[con].values.reshape(len(test),1))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack train/test data\n",
    "X_train = np.hstack(tuple(each for each in train_cols))\n",
    "X_test = np.hstack(tuple(each for each in test_cols))\n",
    "y_train = train['PURCHASE_FLG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.,   1.,   0., ...,   4., 126.,  25.],\n",
       "       [  3.,   1.,   0., ...,   4., 126.,  25.],\n",
       "       [  3.,   1.,   0., ...,   4., 126.,  25.],\n",
       "       ...,\n",
       "       [ 21.,  12.,   0., ...,   3., 178.,  39.],\n",
       "       [ 21.,  12.,   0., ...,   2., 179.,  25.],\n",
       "       [  8.,   4.,   0., ...,   2.,  67.,  53.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.,   2.,   0., ...,   4., 118.,  25.],\n",
       "       [  5.,   2.,   0., ...,   4., 118.,  34.],\n",
       "       [  5.,   2.,   1., ...,   4., 118.,  41.],\n",
       "       ...,\n",
       "       [ 10.,   6.,   0., ...,   4.,  67.,  35.],\n",
       "       [ 10.,   6.,   0., ...,   4.,  67.,  59.],\n",
       "       [ 10.,   6.,   0., ...,   4.,  67.,  38.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2517201    0\n",
       "2517202    0\n",
       "2517203    0\n",
       "2517204    1\n",
       "2517205    1\n",
       "Name: PURCHASE_FLG, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 조건부 확률 모형 : 각 클래스가 정답일 조건부 확률을 계산\n",
    "\n",
    "    - 조건부 확률기반 생성모형 : 베이즈 정리를 사용\n",
    "\n",
    "        - LDA (linear discriminant analysis)\n",
    "        - QDA (Quadratic Discriminanat Analysis)\n",
    "        - 나이브 베이지안 (Naive Bayes)\n",
    "    \n",
    "    - 조건부 확률기반 판별모형 :  직접 조건부 확률 함수를 추정\n",
    "    \n",
    "        - 로지스틱 회귀 (Logistic Regression)\n",
    "        - 의사결정나무 (Descision Tree)\n",
    "        - KNN (K Nearest Neighbor)\n",
    "        \n",
    "        \n",
    "- 판별함수 모형 : 경계면을 찾아서 데이터가 어느 위치에 있는지 계산\n",
    "\n",
    "    - 퍼셉트론 (Perceptron)\n",
    "    - 서포트 벡터 머신 (Support Vector Machine)\n",
    "    - 신경망 (Neural Network)  \n",
    "    \n",
    "    \n",
    "- 모형결합 (Ensemble) : 복수의 예측모형을 결합하여 더 나은 성능을 예측하려는 시도\n",
    "\n",
    "    - 취합 방법론 : 사용할 모형의 집합이 이미 결정되어 있음\n",
    "        \n",
    "        - 다수결 (Majority voting)\n",
    "        - 배깅 (Bagging)\n",
    "        - 랜덤 포레스트 (Random Forest)\n",
    "        \n",
    "    - 부스팅 방법론 : 사용할 모형을 점진적으로 늘림\n",
    "    \n",
    "        - 에이다 부스트 (AdaBoost)\n",
    "        - 그레디언트 부스트 (Gradient Boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 조건부 확률모형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1 조건부 확률기반 생성 모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA (linear discriminant analysis) (score : 0.000758)\n",
    "model = LinearDiscriminantAnalysis(n_components=3, solver=\"svd\", \n",
    "        store_covariance=True).fit(X_train, y_train)\n",
    "predict_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('LDA.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QDA (Quadratic Discriminanat Analysis) (score : 0.000103)\n",
    "model = QuadraticDiscriminantAnalysis().fit(X_train, y_train)\n",
    "predict_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('QDA.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Naive bayesian - Multinomial (score : 0.000512)\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "predict_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('NBM.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2 조건부 확률기반 판별모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression : 사용 X (종속변수가 이항분포를 따라야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descision Tree\n",
    "model = DecisionTreeClassifier(criterion='entropy', \n",
    "        max_depth=7, min_samples_leaf=5).fit(X_train, y_train)\n",
    "predict_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('DT.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN (K Nearest Neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 모형결합 (Ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 취합 방법론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ed6297eca433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Top 10 Coupon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpos_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtop10_coupon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'USER_ID_hash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_merge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtop10_coupon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'PURCHASED_COUPONS'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# 다수결 (Majority voting)\n",
    "\n",
    "# 취합할 모델 생성\n",
    "model1 = LinearDiscriminantAnalysis(n_components=3, solver=\"svd\", store_covariance=True)\n",
    "model2 = QuadraticDiscriminantAnalysis()\n",
    "model3 = GaussianNB()\n",
    "model4 = MultinomialNB()\n",
    "\n",
    "# ensemble 생성\n",
    "ensemble = VotingClassifier(estimators=[('lda', model1), ('qda', model2), ('gnb', model3), ('mul', model4)], \n",
    "                            voting='soft', weights=[1, 1, 1, 1])\n",
    "\n",
    "predict_proba = [c.fit(X_train, y_train).predict_proba(X_test) for c in (model1, model2, model3, model4, ensemble)]\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('ensemble.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배깅 (Bagging) (score : 0.000552)\n",
    "model1 = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "model2 = BaggingClassifier(DecisionTreeClassifier(), bootstrap_features=True, random_state=0).fit(X_train, y_train)\n",
    "predict_proba = model2.predict_proba(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('Bagging.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 랜덤포레스트 (RandomForest) (score : 0.000846)\n",
    "clf = RandomForestClassifier(n_estimators=500, max_depth=5, min_samples_split = 10, criterion = 'entropy')\n",
    "model = clf.fit(X_train, y_train)\n",
    "predict_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('RandomForest.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 부스팅 방법론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이다 부스트 (Ada Boost)\n",
    "model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5, random_state=0), \n",
    "                               algorithm=\"SAMME\", n_estimators=100).fit(X_train, y_train)\n",
    "predict_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('Ada.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그레디언트 부스트 (Gradient Boost)\n",
    "model = GradientBoostingClassifier(n_estimators=100, max_depth=2, random_state=0).fit(X_train, y_train)\n",
    "predict_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('Gradient.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# XG boost\n",
    "model = xgboost.XGBClassifier(n_estimators=200, max_depth=3).fit(X_train, y_train)\n",
    "predict_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('XG1.csv',  header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 판별함수 모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퍼셉트론 (Perceptron) - perceptron\n",
    "model = Perceptron(max_iter=500, eta0=0.1, random_state=1).fit(X_train, y_train)\n",
    "predict_proba = model.predict(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('Perceptron.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퍼셉트론 (Perceptron) - SGD\n",
    "model = SGDClassifier(loss=\"hinge\", max_iter=3, random_state=1).fit(X_train, y_train)\n",
    "predict_proba = model.predict(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('SGD.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "\n",
      " [[281   0   0]\n",
      " [281   0   0]\n",
      " [198   0   0]]\n",
      "\n",
      "\n",
      " Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Lose       0.37      1.00      0.54       281\n",
      "        Win       0.00      0.00      0.00       281\n",
      "       Draw       0.00      0.00      0.00       198\n",
      "\n",
      "avg / total       0.14      0.37      0.20       760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# 서포트 벡터 머신 (Support Vector Machine) - linear\n",
    "model = SVC(kernel='linear').fit(X_train, y_train)\n",
    "predict_proba = model.predict(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('NBM.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "\n",
      " [[281   0   0]\n",
      " [281   0   0]\n",
      " [198   0   0]]\n",
      "\n",
      "\n",
      " Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Lose       0.37      1.00      0.54       281\n",
      "        Win       0.00      0.00      0.00       281\n",
      "       Draw       0.00      0.00      0.00       198\n",
      "\n",
      "avg / total       0.14      0.37      0.20       760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# 서포트 벡터 머신 (Support Vector Machine) - 다항 커널 (Polynomial Kernel)\n",
    "model = SVC(kernel=\"poly\", degree=2, gamma=1, coef0=0).fit(X_train, y_train)\n",
    "predict_proba = model.predict(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('NBM.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "\n",
      " [[281   0   0]\n",
      " [281   0   0]\n",
      " [198   0   0]]\n",
      "\n",
      "\n",
      " Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Lose       0.37      1.00      0.54       281\n",
      "        Win       0.00      0.00      0.00       281\n",
      "       Draw       0.00      0.00      0.00       198\n",
      "\n",
      "avg / total       0.14      0.37      0.20       760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# 서포트 벡터 머신 (Support Vector Machine) - RBF(Radial Basis Function)\n",
    "model = SVC(kernel=\"rbf\").fit(X_train, y_train)\n",
    "predict_proba = model.predict(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('NBM.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "\n",
      " [[281   0   0]\n",
      " [281   0   0]\n",
      " [198   0   0]]\n",
      "\n",
      "\n",
      " Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Lose       0.37      1.00      0.54       281\n",
      "        Win       0.00      0.00      0.00       281\n",
      "       Draw       0.00      0.00      0.00       198\n",
      "\n",
      "avg / total       0.14      0.37      0.20       760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# 서포트 벡터 머신 (Support Vector Machine) - 시그모이드 커널 (Sigmoid Kernel)\n",
    "model = SVC(kernel=\"sigmoid\", gamma=2, coef0=2).fit(X_train, y_train)\n",
    "predict_proba = model.predict(X_test)\n",
    "\n",
    "# Top 10 Coupon\n",
    "pos_idx = np.where(model.classes_ == True)[0][0]\n",
    "test['predict'] = predict_proba[:, pos_idx]\n",
    "top10_coupon = test.groupby('USER_ID_hash').apply(top_merge)\n",
    "top10_coupon.name = 'PURCHASED_COUPONS'\n",
    "top10_coupon.to_csv('NBM.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
